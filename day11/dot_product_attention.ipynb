{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dot_product_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9kAzHqpsgB0804JCt7C0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/100-days-of-code/blob/main/day11/dot_product_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f--AbYmWPiLf",
        "outputId": "8baa2f80-b16f-4960-bb54-3ee95e74e885"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "import textwrap\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "\n",
        "def to_tensor(tensor):\n",
        "    return np.array(tensor)\n",
        "\n",
        "\n",
        "def shape_tensor(tensor, name):\n",
        "    print(f'{name} shape: {tensor.shape}\\n')\n",
        "    print(f'{tensor}\\n')\n",
        "\n",
        "print('Q, K, V arrays must have the same embedding dimensions (number of columns)')\n",
        "print('M array must have the same shape as np.dot(Q, K.T)')\n",
        "print()\n",
        "\n",
        "q = to_tensor([[1, 0, 0, 1], [0, 1, 0, 0]])\n",
        "shape_tensor(q, 'query')\n",
        "k = to_tensor([[1, 2, 3, 1], [4, 5, 6, 5]])\n",
        "shape_tensor(k, 'key')\n",
        "v = to_tensor([[0, 1, 0, 0], [1, 0, 1, 1]])\n",
        "shape_tensor(v, 'value')\n",
        "m = to_tensor([[0, 0], [-1e9, 0]])\n",
        "shape_tensor(m, 'mask')\n",
        "shape_tensor(np.dot(q, k.T), 'dot')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q, K, V arrays must have the same embedding dimensions (number of columns)\n",
            "M array must have the same shape as np.dot(Q, K.T)\n",
            "\n",
            "query shape: (2, 4)\n",
            "\n",
            "[[1 0 0 1]\n",
            " [0 1 0 0]]\n",
            "\n",
            "key shape: (2, 4)\n",
            "\n",
            "[[1 2 3 1]\n",
            " [4 5 6 5]]\n",
            "\n",
            "value shape: (2, 4)\n",
            "\n",
            "[[0 1 0 0]\n",
            " [1 0 1 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n",
            "dot shape: (2, 2)\n",
            "\n",
            "[[2 9]\n",
            " [2 5]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpHVVYMkUWoI"
      },
      "source": [
        "$\\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY68eVjESAc5",
        "outputId": "af6fb5a4-6f54-4e32-e486-0e7d1635ed87"
      },
      "source": [
        "def dot_product_self_attention(q, k, v, m, scale=True):\n",
        "    \"\"\"Args:\n",
        "        q (numpy.ndarray): query representations with shape (L_q, d)\n",
        "        k (numpy.ndarray): key representations with shape (L_k, d)\n",
        "        v (numpy.ndarray): value representations with shape (L_v, d), L_v = L_k\n",
        "        m (numpy.ndarray): attention-mask, attention shape (L_q, L_k)\n",
        "        scale (bool): scale the dot product of the q and transposed k\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: self-attention array for q, k, v arrays with shape (L_q, L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert q.shape[-1] == k.shape[-1] == v.shape[-1], \"Embedding dim's of q, k, v have different shapes\"\n",
        "\n",
        "    # Set depth of the query embedding for scaling down the dot product\n",
        "    if scale: \n",
        "        depth = q.shape[-1]\n",
        "    else:\n",
        "        depth = 1\n",
        "\n",
        "    # Scaled query key dot product\n",
        "    dots = np.matmul(q, np.swapaxes(key, -1, -2)) / np.sqrt(depth) \n",
        "    \n",
        "    # Masking\n",
        "    if mask is not None:\n",
        "        dots = np.where(m, dots, np.full_like(dots, -1e9)) \n",
        "    \n",
        "    # Softmax\n",
        "    dots = np.exp(dots - scipy.special.logsumexp(dots, axis=-1, keepdims=True))\n",
        "\n",
        "    # Dots * value\n",
        "    self_attention = np.matmul(dots, v)\n",
        "    \n",
        "    return self_attention\n",
        "\n",
        "\n",
        "def masked_dot_product_self_attention(q, k, v, scale=True):\n",
        "    \"\"\" Returns:\n",
        "        numpy.ndarray: masked dot product self attention tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    # Penultimate q dim\n",
        "    m_size = q.shape[-2]\n",
        "\n",
        "    # Matrix (see fig. above) with shape (1, mask_size, mask_size)\n",
        "    m = np.tril(np.ones((1, m_size, m_size), dtype=np.bool_), k=0)  \n",
        "        \n",
        "    return dot_product_attention(q, k, v, m, scale=scale)\n",
        "\n",
        "masked_dot_product_self_attention(q, k, v)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.        , 1.        , 0.        , 0.        ],\n",
              "        [0.81757448, 0.18242552, 0.81757448, 0.81757448]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDdAXThXc1I2"
      },
      "source": [
        "<img src=\"https://sun9-24.userapi.com/impg/W8AgaZFgXYAlb-XrEs-WeorsQrxtObPz7sj_gw/0T6AULfoIU0.jpg?size=682x522&quality=96&sign=c1ee9debbc950b20937439b674c09e5b&type=album\" width=\"500\"/>\n"
      ]
    }
  ]
}