{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reformer_Efficient_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNiUdhyGs18Wd1KTmLQ+Y0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/100-days-of-code/blob/main/day15/Reformer_Efficient_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P98RGu1-gyYo"
      },
      "source": [
        "* *Reversible Layers* reduce memory \n",
        "* *Locality Sensitive Hashing (LSH)* reduces the cost of the Dot Product attention for large input sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0GxBMbvgCu-",
        "outputId": "be784c91-70d2-4963-c9d6-beea55b164a3"
      },
      "source": [
        "!pip install trax\n",
        "\n",
        "import os\n",
        "import trax\n",
        "from trax import layers as tl \n",
        "import jax\n",
        "from trax import fastmath\n",
        "fastmath.use_backend('tensorflow-numpy')\n",
        "import functools\n",
        "from trax.fastmath import numpy as np\n",
        "from jax.lax import tie_in\n",
        "from trax.layers import (\n",
        "    length_normalized,\n",
        "    apply_broadcasted_dropout,\n",
        "    look_adjacent,\n",
        "    permute_via_gather,\n",
        "    permute_via_sort,\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/51/305b839f51d53abb393777f743e497d27bb341478f3fdec4d6ddaccc9fb5/trax-1.3.7-py2.py3-none-any.whl (521kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/c0fed4301f592c3b56638ae7292612c17d91a43891ba1aaf9636d535beae/tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.65+cuda110)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/83/376533337f39711929bb3f5c2263bfec4bf54abe5f2f1987f3ddf2e10a76/t5-0.9.0-py3-none-any.whl (230kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.12)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hCollecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 35.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.0)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/4b/01fe41784cf0edd8264fbc6fb90750fdaf365441492c425b6ce693c4122f/tfds_nightly-4.2.0.dev202104230108-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.8.1+cu101)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/10/37df0bc87ebf84e1414613176340e3aadc3697d2bd112bf63d3d4b1e848a/mesh_tensorflow-0.1.19-py3-none-any.whl (366kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.29.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (56.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n",
            "Installing collected packages: tensorflow-text, rouge-score, portalocker, sacrebleu, sacremoses, tokenizers, transformers, sentencepiece, tfds-nightly, mesh-tensorflow, t5, funcsigs, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.19 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 t5-0.9.0 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202104230108 tokenizers-0.10.2 transformers-4.5.1 trax-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72I5IOTXoUt7"
      },
      "source": [
        "def mask_self_attention(dots, q_info, kv_info, causal=True, exclude_self=True, masked=False):\n",
        "    if causal:\n",
        "        mask = fastmath.lt(q_info, kv_info).astype(np.float32)\n",
        "        dots = dots - 1e9 * mask\n",
        "    if exclude_self:\n",
        "        mask = np.equal(q_info, kv_info).astype(np.float32)\n",
        "        dots = dots - 1e5 * mask\n",
        "    if masked:\n",
        "        zeros_like_kv_info = tie_in(kv_info, np.zeros_like(kv_info))\n",
        "        mask = fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)\n",
        "        dots = dots - 1e9 * mask\n",
        "    return dots"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eWOjXxqwfSR"
      },
      "source": [
        "Softmax\n",
        "$$ softmax(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n",
        "___\n",
        "\n",
        "Alternative softmax calculation\n",
        "$$ logsumexp(x)=\\log{({\\sum_j \\exp(x_j)})}$$\n",
        "$$ softmax(x_i)=\\exp({x_i - logsumexp(x)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy1ZYWIOwIhj",
        "outputId": "d2b9915c-848e-444c-fb73-efcdbe714fb9"
      },
      "source": [
        "def softmax(x, passthrough=False):\n",
        "    logsumexp = fastmath.logsumexp(x, axis=-1, keepdims=True)\n",
        "    o = np.exp(x - logsumexp)\n",
        "    if passthrough:\n",
        "        return (x, np.zeros_like(logsumexp))\n",
        "    else:\n",
        "        return (o, logsumexp)\n",
        "\n",
        "# Compare two softmax calculation methods\n",
        "test = np.array([4.0, 3.0, 2.0, 1.0])\n",
        "print('The first method: ', np.exp(test) / sum(np.exp(test)))\n",
        "print('The second method:', softmax(test)[0])\n",
        "print('Logsumexp is', softmax(test)[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first method:  [0.6439142  0.2368828  0.08714432 0.0320586 ]\n",
            "The second method: [0.64391416 0.23688279 0.08714431 0.0320586 ]\n",
            "Logsumexp is [4.44019]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97dXovzuzfsT",
        "outputId": "a61a0e8a-2cd7-4de0-9703-205182c3d312"
      },
      "source": [
        "def attend(\n",
        "    q,\n",
        "    k=None,\n",
        "    v=None,\n",
        "    mask_fn=None,\n",
        "    q_info=None,\n",
        "    kv_info=None,\n",
        "    dropout=0.0,\n",
        "    rng=None,\n",
        "    verbose=False,\n",
        "    passthrough=False,\n",
        "):\n",
        "    assert v is not None\n",
        "    share_qk = k is None\n",
        "    if share_qk:\n",
        "        k = q\n",
        "        if kv_info is None:\n",
        "            kv_info = q_info\n",
        "\n",
        "    if share_qk:\n",
        "        k = length_normalized(k)\n",
        "    k = k / np.sqrt(k.shape[-1])\n",
        "\n",
        "    # Dot-product attention.\n",
        "    kr = np.swapaxes(k, -1, -2)  # k transpose\n",
        "    dots = np.matmul(q, kr)\n",
        "    if verbose:\n",
        "        print(\"Dots\", dots.shape)\n",
        "\n",
        "    # Masking\n",
        "    if mask_fn is not None:\n",
        "        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n",
        "\n",
        "    # Softmax.\n",
        "    dots, dots_logsumexp = softmax(dots)\n",
        "    if verbose:\n",
        "        print(\"Attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n",
        "\n",
        "    if dropout > 0.0:\n",
        "        assert rng is not None\n",
        "        # Dropout is broadcast across the bin dimension\n",
        "        dropout_shape = (dots.shape[-2], dots.shape[-1])\n",
        "        keep_prob = tie_in(dots, 1.0 - dropout)\n",
        "        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n",
        "        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n",
        "        dots = dots * multiplier\n",
        "\n",
        "    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n",
        "    out = np.matmul(dots, v)\n",
        "    if verbose:\n",
        "        print(\"Attend out1\", out.shape)\n",
        "    out = np.reshape(out, (-1, out.shape[-1]))\n",
        "    if verbose:\n",
        "        print(\"Attend out2\", out.shape)\n",
        "    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n",
        "    return out, dots_logsumexp\n",
        "\n",
        "\n",
        "seq_len = 10\n",
        "emb_len = 5\n",
        "d_qk = 2\n",
        "d_v = 4\n",
        "with fastmath.use_backend(\"jax\"):  # the backend \n",
        "    rng_attend = fastmath.random.get_prng(1)\n",
        "    q = k = jax.random.uniform(rng_attend, (seq_len, d_qk), dtype=np.float32)\n",
        "    v = jax.random.uniform(rng_attend, (seq_len, d_v), dtype=np.float32)\n",
        "    o, logits = attend(\n",
        "        q,\n",
        "        k,\n",
        "        v,\n",
        "        mask_fn=None,\n",
        "        q_info=None,\n",
        "        kv_info=None,\n",
        "        dropout=0.0,\n",
        "        rng=rng_attend,\n",
        "        verbose=True,\n",
        "    )\n",
        "print(o, \"\\n\", logits)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "[[0.41215032 0.4410399  0.5783373  0.5694611 ]\n",
            " [0.4146571  0.44229436 0.5821161  0.5674372 ]\n",
            " [0.41805273 0.44460416 0.58764815 0.5645627 ]\n",
            " [0.41195232 0.44039991 0.5797606  0.5747733 ]\n",
            " [0.4137767  0.44150043 0.5821494  0.5723262 ]\n",
            " [0.4117964  0.4412175  0.5771261  0.5673998 ]\n",
            " [0.41744018 0.44462895 0.5903716  0.57462746]\n",
            " [0.4107483  0.44038767 0.5765146  0.57118237]\n",
            " [0.41690123 0.44381702 0.5880829  0.5720327 ]\n",
            " [0.41648537 0.44338638 0.5865165  0.570186  ]] \n",
            " [2.3861837 2.4581163 2.5707448 2.4802527 2.5137725 2.33331   2.755928\n",
            " 2.36104   2.6662705 2.6040623]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvpGOhbJNr5D",
        "outputId": "0d797258-5056-4236-be43-8eaaa6d90715"
      },
      "source": [
        "class SelfAttention(tl.SelfAttention):\n",
        "    def forward_unbatched(\n",
        "        self, x, mask=None, *, weights, state, rng, update_state, verbose=False\n",
        "    ):\n",
        "        del update_state\n",
        "        attend_rng, output_rng = fastmath.random.split(rng)\n",
        "        if self.bias:\n",
        "            if self.share_qk:\n",
        "                w_q, w_v, w_o, b_q, b_v = weights\n",
        "            else:\n",
        "                w_q, w_k, w_v, w_o, b_q, b_k, b_v = weights\n",
        "        else:\n",
        "            if self.share_qk:\n",
        "                w_q, w_v, w_o = weights\n",
        "            else:\n",
        "                w_q, w_k, w_v, w_o = weights\n",
        "\n",
        "        print(\"x.shape, w_q.shape\", x.shape, w_q.shape)\n",
        "        q = np.matmul(x, w_q)\n",
        "        k = None\n",
        "        if not self.share_qk:\n",
        "            k = np.matmul(x, w_k)\n",
        "        v = np.matmul(x, w_v)\n",
        "\n",
        "        if self.bias:\n",
        "            q = q + b_q\n",
        "            if not self.share_qk:\n",
        "                k = k + b_k\n",
        "            v = v + b_v\n",
        "\n",
        "        mask_fn = functools.partial(\n",
        "            mask_self_attention,\n",
        "            causal=self.causal,\n",
        "            exclude_self=self.share_qk,\n",
        "            masked=self.masked,\n",
        "        )\n",
        "        q_info = kv_info = tie_in(x, np.arange(q.shape[-2], dtype=np.int32))\n",
        "\n",
        "        assert (mask is not None) == self.masked\n",
        "        if self.masked:\n",
        "            # mask is a boolean array (True means \"is valid token\")\n",
        "            ones_like_mask = tie_in(x, np.ones_like(mask, dtype=np.int32))\n",
        "            kv_info = kv_info * np.where(mask, ones_like_mask, -ones_like_mask)\n",
        "\n",
        "        o, _ = attend(\n",
        "            q,\n",
        "            k,\n",
        "            v,\n",
        "            mask_fn=mask_fn,\n",
        "            q_info=q_info,\n",
        "            kv_info=kv_info,\n",
        "            dropout=self.attention_dropout,\n",
        "            rng=attend_rng,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        # wo weight matrix applied to output of attend in forward_unbatched\n",
        "        out = np.matmul(o, w_o)\n",
        "        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n",
        "        return out, state\n",
        "\n",
        "\n",
        "causal = False\n",
        "masked = False\n",
        "mask = None\n",
        "attention_dropout = 0.01\n",
        "n_heads = 12\n",
        "d_qk = 2\n",
        "d_v = 4\n",
        "seq_len = 10\n",
        "emb_len = 5\n",
        "batch_size = 16\n",
        "\n",
        "osa = SelfAttention(\n",
        "    n_heads=n_heads,\n",
        "    d_qk=d_qk,\n",
        "    d_v=d_v,\n",
        "    causal=causal,\n",
        "    use_reference_code=True,\n",
        "    attention_dropout=attention_dropout,\n",
        "    mode=\"train\",\n",
        ")\n",
        "\n",
        "rng_osa = fastmath.random.get_prng(1)\n",
        "x = jax.random.uniform(\n",
        "    jax.random.PRNGKey(0), (batch_size, seq_len, emb_len), dtype=np.float32\n",
        ")\n",
        "_, _ = osa.init(tl.shapes.signature(x), rng=rng_osa)\n",
        "\n",
        "osa(x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n",
            "x.shape, w_q.shape (10, 5) (5, 2)\n",
            "Dots (10, 10)\n",
            "Attend dots post softmax (10, 10) (10, 1)\n",
            "Attend out1 (10, 4)\n",
            "Attend out2 (10, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[ 2.70541877e-01, -1.91065639e-01, -9.75493491e-02,\n",
              "               -4.98708904e-01, -9.28128883e-02],\n",
              "              [ 2.70205379e-01, -1.89427227e-01, -9.98546556e-02,\n",
              "               -4.99432772e-01, -9.20516998e-02],\n",
              "              [ 2.67894775e-01, -1.87397450e-01, -9.83914435e-02,\n",
              "               -5.02314806e-01, -8.69192630e-02],\n",
              "              [ 2.69780993e-01, -1.88655436e-01, -9.91298556e-02,\n",
              "               -5.00778019e-01, -8.91079381e-02],\n",
              "              [ 2.64413923e-01, -1.61305130e-01, -9.35228318e-02,\n",
              "               -4.34522957e-01, -9.77130830e-02],\n",
              "              [ 2.68971711e-01, -1.87803984e-01, -1.00712404e-01,\n",
              "               -5.01329243e-01, -8.79787207e-02],\n",
              "              [ 2.72200435e-01, -1.90876096e-01, -9.56467688e-02,\n",
              "               -4.98503745e-01, -9.28032100e-02],\n",
              "              [ 2.18693882e-01, -1.79120436e-01, -9.83639807e-02,\n",
              "               -4.28791493e-01, -1.08542286e-01],\n",
              "              [ 2.65745878e-01, -1.86937898e-01, -1.03170246e-01,\n",
              "               -5.01813412e-01, -8.87570605e-02],\n",
              "              [ 2.66099274e-01, -1.88568652e-01, -1.00020975e-01,\n",
              "               -5.01172662e-01, -8.94977450e-02]],\n",
              "\n",
              "             [[ 3.46339285e-01, -2.44380176e-01, -5.35441786e-02,\n",
              "               -4.27222371e-01, -3.92149091e-02],\n",
              "              [ 3.42004716e-01, -2.39982396e-01, -4.94649708e-02,\n",
              "               -4.35668200e-01, -3.24202031e-02],\n",
              "              [ 3.42744082e-01, -2.40193486e-01, -5.08278608e-02,\n",
              "               -4.37226832e-01, -3.03702131e-02],\n",
              "              [ 3.44545543e-01, -2.43198037e-01, -5.09344041e-02,\n",
              "               -4.34003621e-01, -3.75224128e-02],\n",
              "              [ 3.25558841e-01, -2.37639427e-01, -4.83907312e-02,\n",
              "               -3.62903178e-01, -5.59833571e-02],\n",
              "              [ 3.47844273e-01, -2.39637256e-01, -5.36028147e-02,\n",
              "               -4.34315234e-01, -3.43980156e-02],\n",
              "              [ 3.40487182e-01, -2.42488295e-01, -4.95018587e-02,\n",
              "               -4.31509823e-01, -3.38215977e-02],\n",
              "              [ 3.24460953e-01, -2.20966473e-01, -5.56611195e-02,\n",
              "               -3.88184935e-01, -6.25353456e-02],\n",
              "              [ 3.44156802e-01, -2.40225241e-01, -5.15753403e-02,\n",
              "               -4.36033607e-01, -3.18475589e-02],\n",
              "              [ 3.45189214e-01, -2.46118844e-01, -5.37284091e-02,\n",
              "               -4.26627249e-01, -3.87470312e-02]],\n",
              "\n",
              "             [[ 2.15364784e-01, -1.22781292e-01, -1.38330787e-01,\n",
              "               -5.09611726e-01, -1.19683757e-01],\n",
              "              [ 2.15226978e-01, -1.25098199e-01, -1.37319863e-01,\n",
              "               -5.07479548e-01, -1.20881274e-01],\n",
              "              [ 2.17805550e-01, -1.22096330e-01, -1.42285123e-01,\n",
              "               -5.10681272e-01, -1.21542230e-01],\n",
              "              [ 2.13474303e-01, -1.23522155e-01, -1.32470071e-01,\n",
              "               -5.11207521e-01, -1.17468700e-01],\n",
              "              [ 2.09080324e-01, -9.79819149e-02, -1.21485330e-01,\n",
              "               -4.46637094e-01, -1.14964522e-01],\n",
              "              [ 2.20317706e-01, -1.22252509e-01, -1.38847694e-01,\n",
              "               -5.07993460e-01, -1.22814089e-01],\n",
              "              [ 2.10641697e-01, -1.20643005e-01, -1.32865861e-01,\n",
              "               -5.05142152e-01, -1.17048785e-01],\n",
              "              [ 1.95550829e-01, -1.13730997e-01, -1.27002209e-01,\n",
              "               -4.77740109e-01, -1.05276130e-01],\n",
              "              [ 2.18997002e-01, -1.24047317e-01, -1.38439879e-01,\n",
              "               -5.09498358e-01, -1.22907430e-01],\n",
              "              [ 2.10455865e-01, -1.25144362e-01, -1.34859547e-01,\n",
              "               -5.06699085e-01, -1.17802985e-01]],\n",
              "\n",
              "             [[ 2.70737588e-01, -2.26398483e-01, -9.97599065e-02,\n",
              "               -5.50272346e-01, -1.26751065e-01],\n",
              "              [ 2.81667352e-01, -2.21576542e-01, -1.04283571e-01,\n",
              "               -5.53047538e-01, -1.31910816e-01],\n",
              "              [ 2.69829035e-01, -2.26213574e-01, -9.87089127e-02,\n",
              "               -5.50218284e-01, -1.26983762e-01],\n",
              "              [ 2.75665402e-01, -2.23515183e-01, -1.00122459e-01,\n",
              "               -5.55047631e-01, -1.26176625e-01],\n",
              "              [ 2.38779813e-01, -1.97261289e-01, -9.65044349e-02,\n",
              "               -4.82374728e-01, -1.09423213e-01],\n",
              "              [ 2.77530283e-01, -2.19635695e-01, -1.00642666e-01,\n",
              "               -5.55177450e-01, -1.26303628e-01],\n",
              "              [ 2.73673207e-01, -2.22096160e-01, -1.05480060e-01,\n",
              "               -5.51572680e-01, -1.28875732e-01],\n",
              "              [ 2.31231406e-01, -1.95533961e-01, -9.73661318e-02,\n",
              "               -4.65661705e-01, -1.21813700e-01],\n",
              "              [ 2.72919714e-01, -2.22601920e-01, -9.55562592e-02,\n",
              "               -5.52264035e-01, -1.25306040e-01],\n",
              "              [ 2.71645755e-01, -2.21283033e-01, -1.00037135e-01,\n",
              "               -5.53007483e-01, -1.23741552e-01]],\n",
              "\n",
              "             [[ 2.37914860e-01, -2.03259498e-01, -1.27315789e-01,\n",
              "               -4.18052346e-01, -1.52773127e-01],\n",
              "              [ 2.32892379e-01, -2.02670157e-01, -1.26864046e-01,\n",
              "               -4.21004564e-01, -1.48509353e-01],\n",
              "              [ 2.38617957e-01, -2.00238794e-01, -1.22091711e-01,\n",
              "               -4.19379652e-01, -1.49018601e-01],\n",
              "              [ 2.38900632e-01, -1.98738903e-01, -1.20857805e-01,\n",
              "               -4.17815924e-01, -1.49469405e-01],\n",
              "              [ 2.44449049e-01, -1.91467375e-01, -9.82031524e-02,\n",
              "               -3.62763137e-01, -1.23207480e-01],\n",
              "              [ 2.37584457e-01, -2.02546597e-01, -1.20074168e-01,\n",
              "               -4.13337767e-01, -1.51302263e-01],\n",
              "              [ 2.32317954e-01, -2.01229453e-01, -1.25514001e-01,\n",
              "               -4.21768337e-01, -1.46947652e-01],\n",
              "              [ 2.05534905e-01, -1.73938692e-01, -1.16413787e-01,\n",
              "               -3.51082355e-01, -1.85226232e-01],\n",
              "              [ 2.41171256e-01, -1.99698582e-01, -1.21598698e-01,\n",
              "               -4.16741014e-01, -1.50511160e-01],\n",
              "              [ 2.34618843e-01, -2.02026725e-01, -1.27836064e-01,\n",
              "               -4.18910414e-01, -1.48787871e-01]],\n",
              "\n",
              "             [[ 2.84776688e-01, -2.19463870e-01, -6.38285577e-02,\n",
              "               -5.05847514e-01,  4.18911874e-03],\n",
              "              [ 2.78437138e-01, -2.12500796e-01, -6.02223501e-02,\n",
              "               -5.08848965e-01,  1.33421384e-02],\n",
              "              [ 2.87029445e-01, -2.21497118e-01, -6.26695305e-02,\n",
              "               -5.02128363e-01,  5.52206486e-03],\n",
              "              [ 2.87208527e-01, -2.19185665e-01, -6.16037995e-02,\n",
              "               -5.03477097e-01,  4.61169705e-03],\n",
              "              [ 2.61063844e-01, -1.95465058e-01, -4.11353856e-02,\n",
              "               -4.48041618e-01, -7.06932321e-03],\n",
              "              [ 2.82673508e-01, -2.20416397e-01, -6.13862723e-02,\n",
              "               -5.02108991e-01,  3.98984179e-03],\n",
              "              [ 2.80708343e-01, -2.21299678e-01, -6.28769845e-02,\n",
              "               -5.04924834e-01,  1.01040229e-02],\n",
              "              [ 2.74112105e-01, -2.01551393e-01, -3.90232876e-02,\n",
              "               -4.43856537e-01,  5.06415442e-02],\n",
              "              [ 2.82479107e-01, -2.17872247e-01, -5.78126013e-02,\n",
              "               -5.04865408e-01,  7.10039213e-03],\n",
              "              [ 2.79710591e-01, -2.22695723e-01, -6.03622496e-02,\n",
              "               -4.99856412e-01,  2.63403356e-03]],\n",
              "\n",
              "             [[ 3.33296001e-01, -1.80558741e-01, -6.21510595e-02,\n",
              "               -5.81741452e-01, -5.01941368e-02],\n",
              "              [ 3.33754539e-01, -1.86166316e-01, -6.55085891e-02,\n",
              "               -5.84938347e-01, -5.67124784e-02],\n",
              "              [ 3.32839191e-01, -1.80252865e-01, -6.23081625e-02,\n",
              "               -5.81170738e-01, -4.81564254e-02],\n",
              "              [ 3.30246150e-01, -1.81992739e-01, -5.98569214e-02,\n",
              "               -5.82873940e-01, -5.04704937e-02],\n",
              "              [ 2.79558748e-01, -1.69405624e-01, -6.87705874e-02,\n",
              "               -5.13129354e-01, -5.62073924e-02],\n",
              "              [ 3.37546319e-01, -1.79200083e-01, -6.49485886e-02,\n",
              "               -5.82710505e-01, -4.99228686e-02],\n",
              "              [ 3.29873592e-01, -1.86376780e-01, -6.02972135e-02,\n",
              "               -5.87598026e-01, -5.76050580e-02],\n",
              "              [ 3.01820487e-01, -1.64867610e-01, -4.26241532e-02,\n",
              "               -4.79621768e-01, -2.05346681e-02],\n",
              "              [ 3.33483338e-01, -1.84832573e-01, -6.40716702e-02,\n",
              "               -5.85503221e-01, -5.59868664e-02],\n",
              "              [ 3.34670633e-01, -1.80198893e-01, -6.22043908e-02,\n",
              "               -5.83723903e-01, -5.24671003e-02]],\n",
              "\n",
              "             [[ 2.38449290e-01, -1.79566547e-01, -1.19274095e-01,\n",
              "               -4.38668847e-01, -1.15472414e-01],\n",
              "              [ 2.37731099e-01, -1.74571991e-01, -1.21206701e-01,\n",
              "               -4.41629201e-01, -1.09659962e-01],\n",
              "              [ 2.45933667e-01, -1.78936765e-01, -1.24975488e-01,\n",
              "               -4.35760885e-01, -1.17725037e-01],\n",
              "              [ 2.38509864e-01, -1.77925169e-01, -1.17957503e-01,\n",
              "               -4.36570555e-01, -1.15180328e-01],\n",
              "              [ 1.99767977e-01, -1.48603693e-01, -1.02434747e-01,\n",
              "               -3.68994176e-01, -1.02082372e-01],\n",
              "              [ 2.35953912e-01, -1.72342092e-01, -1.16859213e-01,\n",
              "               -4.43576574e-01, -1.11779399e-01],\n",
              "              [ 2.40859747e-01, -1.78359568e-01, -1.20433167e-01,\n",
              "               -4.37166512e-01, -1.16083853e-01],\n",
              "              [ 2.01260775e-01, -1.51597843e-01, -1.06935829e-01,\n",
              "               -4.17400241e-01, -9.12314057e-02],\n",
              "              [ 2.42021263e-01, -1.73004895e-01, -1.22581497e-01,\n",
              "               -4.41096127e-01, -1.13173567e-01],\n",
              "              [ 2.40633935e-01, -1.75087482e-01, -1.18266836e-01,\n",
              "               -4.36749786e-01, -1.15864351e-01]],\n",
              "\n",
              "             [[ 3.05555642e-01, -2.14075446e-01, -2.67164707e-02,\n",
              "               -4.86714959e-01,  9.04609561e-02],\n",
              "              [ 3.02554786e-01, -2.12984979e-01, -2.89909691e-02,\n",
              "               -4.90684509e-01,  9.39216763e-02],\n",
              "              [ 3.00824523e-01, -2.07705483e-01, -2.22342014e-02,\n",
              "               -4.88561571e-01,  9.58769023e-02],\n",
              "              [ 3.05421054e-01, -2.14866668e-01, -3.13503295e-02,\n",
              "               -4.91634965e-01,  9.33481157e-02],\n",
              "              [ 2.57638007e-01, -2.12626934e-01, -1.62055641e-02,\n",
              "               -4.31627959e-01,  1.15781076e-01],\n",
              "              [ 3.05420578e-01, -2.09806442e-01, -2.76903138e-02,\n",
              "               -4.91539180e-01,  9.79007855e-02],\n",
              "              [ 3.04069787e-01, -2.13434070e-01, -2.83670127e-02,\n",
              "               -4.88959074e-01,  9.22284871e-02],\n",
              "              [ 2.36946434e-01, -1.58247799e-01, -3.53223160e-02,\n",
              "               -4.57828075e-01,  7.88113773e-02],\n",
              "              [ 3.03083032e-01, -2.11997420e-01, -2.96318009e-02,\n",
              "               -4.90249038e-01,  9.58157480e-02],\n",
              "              [ 3.10613245e-01, -2.13388309e-01, -2.75352374e-02,\n",
              "               -4.88747597e-01,  9.33984816e-02]],\n",
              "\n",
              "             [[ 2.92327642e-01, -1.84366703e-01, -1.35672823e-01,\n",
              "               -4.81395006e-01, -1.77722767e-01],\n",
              "              [ 2.90046036e-01, -1.83900207e-01, -1.38018966e-01,\n",
              "               -4.81030524e-01, -1.77128956e-01],\n",
              "              [ 2.87475675e-01, -1.84852928e-01, -1.42644137e-01,\n",
              "               -4.81029838e-01, -1.75015181e-01],\n",
              "              [ 2.85346836e-01, -1.81804791e-01, -1.43787220e-01,\n",
              "               -4.83210415e-01, -1.74193650e-01],\n",
              "              [ 2.75321603e-01, -1.50411412e-01, -1.19291142e-01,\n",
              "               -4.19763118e-01, -1.69403970e-01],\n",
              "              [ 2.87264228e-01, -1.82289124e-01, -1.41680866e-01,\n",
              "               -4.82994616e-01, -1.71097130e-01],\n",
              "              [ 2.85308659e-01, -1.87336281e-01, -1.39060259e-01,\n",
              "               -4.80706811e-01, -1.79024339e-01],\n",
              "              [ 2.75363535e-01, -1.77521154e-01, -1.16374239e-01,\n",
              "               -4.16414827e-01, -1.31101057e-01],\n",
              "              [ 2.88117290e-01, -1.87715501e-01, -1.39223680e-01,\n",
              "               -4.78988290e-01, -1.79920733e-01],\n",
              "              [ 2.83896029e-01, -1.85388222e-01, -1.40648663e-01,\n",
              "               -4.79489237e-01, -1.76840648e-01]],\n",
              "\n",
              "             [[ 3.44347864e-01, -2.33010530e-01, -1.10451669e-01,\n",
              "               -5.70794642e-01, -1.03134722e-01],\n",
              "              [ 3.45688403e-01, -2.28688151e-01, -1.16228178e-01,\n",
              "               -5.70384502e-01, -9.81382430e-02],\n",
              "              [ 3.51017386e-01, -2.28301093e-01, -1.19909115e-01,\n",
              "               -5.69814742e-01, -1.01094052e-01],\n",
              "              [ 3.48421037e-01, -2.25832790e-01, -1.23503350e-01,\n",
              "               -5.67875206e-01, -9.58243087e-02],\n",
              "              [ 3.12339008e-01, -2.15145409e-01, -1.03699416e-01,\n",
              "               -5.44918299e-01, -9.87247527e-02],\n",
              "              [ 3.46078575e-01, -2.38209009e-01, -1.13489054e-01,\n",
              "               -5.70540428e-01, -1.08411632e-01],\n",
              "              [ 3.46637070e-01, -2.25347728e-01, -1.18079260e-01,\n",
              "               -5.70373654e-01, -9.34606567e-02],\n",
              "              [ 3.27834249e-01, -2.31967717e-01, -9.63362604e-02,\n",
              "               -5.06699800e-01, -1.09700538e-01],\n",
              "              [ 3.42510879e-01, -2.32710540e-01, -1.10645622e-01,\n",
              "               -5.71843326e-01, -1.01815663e-01],\n",
              "              [ 3.47266257e-01, -2.36420006e-01, -1.13665387e-01,\n",
              "               -5.72258234e-01, -1.08787715e-01]],\n",
              "\n",
              "             [[ 2.63230741e-01, -1.68294251e-01, -1.22743778e-01,\n",
              "               -4.97888833e-01, -7.44660646e-02],\n",
              "              [ 2.60692835e-01, -1.71156183e-01, -1.19078368e-01,\n",
              "               -5.03738463e-01, -7.79825523e-02],\n",
              "              [ 2.62451112e-01, -1.68861896e-01, -1.17359519e-01,\n",
              "               -5.03435850e-01, -7.72807747e-02],\n",
              "              [ 2.66296059e-01, -1.68815777e-01, -1.21005565e-01,\n",
              "               -4.98919845e-01, -8.15425068e-02],\n",
              "              [ 2.35778600e-01, -1.57507241e-01, -9.62233841e-02,\n",
              "               -4.13127989e-01, -4.71802279e-02],\n",
              "              [ 2.67147362e-01, -1.68379277e-01, -1.21486135e-01,\n",
              "               -4.99354154e-01, -8.00951719e-02],\n",
              "              [ 2.65456617e-01, -1.68504596e-01, -1.17801249e-01,\n",
              "               -4.96599853e-01, -7.96078667e-02],\n",
              "              [ 2.42717355e-01, -1.66328624e-01, -1.01635560e-01,\n",
              "               -4.03662592e-01, -5.38269393e-02],\n",
              "              [ 2.63834059e-01, -1.66602463e-01, -1.14635095e-01,\n",
              "               -4.99516696e-01, -7.65083060e-02],\n",
              "              [ 2.66203612e-01, -1.64667308e-01, -1.20082781e-01,\n",
              "               -4.96286988e-01, -7.61144608e-02]],\n",
              "\n",
              "             [[ 3.24123979e-01, -2.52235025e-01, -7.63914660e-02,\n",
              "               -3.89197767e-01, -5.41897491e-03],\n",
              "              [ 3.26219410e-01, -2.53873289e-01, -7.99115077e-02,\n",
              "               -3.88234615e-01, -6.72844797e-03],\n",
              "              [ 3.27748537e-01, -2.52530396e-01, -7.60178491e-02,\n",
              "               -3.90556872e-01, -6.89675659e-03],\n",
              "              [ 3.24161232e-01, -2.52353847e-01, -8.02758485e-02,\n",
              "               -3.83922040e-01, -7.14504346e-03],\n",
              "              [ 2.96537250e-01, -2.19437644e-01, -8.38906989e-02,\n",
              "               -3.79067689e-01, -3.85015048e-02],\n",
              "              [ 3.36095542e-01, -2.50459373e-01, -8.28103125e-02,\n",
              "               -3.87043446e-01, -7.62025267e-03],\n",
              "              [ 3.31486940e-01, -2.55264163e-01, -8.07239264e-02,\n",
              "               -3.86979043e-01, -1.15985498e-02],\n",
              "              [ 3.08683366e-01, -2.42285237e-01, -6.96549118e-02,\n",
              "               -3.58981133e-01,  3.56529281e-03],\n",
              "              [ 3.31868231e-01, -2.54024446e-01, -8.15540999e-02,\n",
              "               -3.85987610e-01, -1.06537305e-02],\n",
              "              [ 3.28077793e-01, -2.55685210e-01, -7.87459910e-02,\n",
              "               -3.90581906e-01, -9.39977914e-03]],\n",
              "\n",
              "             [[ 2.09611163e-01, -2.36995220e-01, -6.97153360e-02,\n",
              "               -4.31402475e-01, -2.44133174e-04],\n",
              "              [ 2.12359399e-01, -2.35105455e-01, -7.15213716e-02,\n",
              "               -4.31841165e-01, -1.09614059e-03],\n",
              "              [ 2.09047943e-01, -2.37830579e-01, -6.88947067e-02,\n",
              "               -4.28861469e-01,  1.10119581e-05],\n",
              "              [ 2.17774168e-01, -2.34325200e-01, -6.96189106e-02,\n",
              "               -4.32689011e-01, -3.04489210e-03],\n",
              "              [ 1.99776992e-01, -2.28155941e-01, -6.37695789e-02,\n",
              "               -4.11070764e-01, -8.92776623e-03],\n",
              "              [ 2.15370953e-01, -2.24041179e-01, -7.71448612e-02,\n",
              "               -4.34941500e-01,  2.40457803e-03],\n",
              "              [ 2.09920347e-01, -2.31643453e-01, -7.22360462e-02,\n",
              "               -4.34088409e-01,  3.59695032e-03],\n",
              "              [ 1.65010214e-01, -1.92830592e-01, -7.08684251e-02,\n",
              "               -3.68784726e-01,  6.35667145e-03],\n",
              "              [ 2.18567073e-01, -2.33895689e-01, -7.04610944e-02,\n",
              "               -4.32253659e-01, -3.77276912e-03],\n",
              "              [ 2.15574801e-01, -2.34589249e-01, -7.27343112e-02,\n",
              "               -4.33837533e-01, -3.83582339e-03]],\n",
              "\n",
              "             [[ 3.38947177e-01, -2.26061463e-01, -5.14328033e-02,\n",
              "               -4.45084393e-01,  5.97576387e-02],\n",
              "              [ 3.40709835e-01, -2.35312074e-01, -4.81142849e-02,\n",
              "               -4.44574356e-01,  5.23175374e-02],\n",
              "              [ 3.42040688e-01, -2.28221357e-01, -5.23793548e-02,\n",
              "               -4.46238071e-01,  5.41682616e-02],\n",
              "              [ 3.44063938e-01, -2.30627939e-01, -5.52262440e-02,\n",
              "               -4.47508842e-01,  5.22070676e-02],\n",
              "              [ 2.98008680e-01, -1.92946881e-01, -6.19621649e-02,\n",
              "               -4.37502563e-01,  1.81313790e-02],\n",
              "              [ 3.44328552e-01, -2.24213749e-01, -5.37437648e-02,\n",
              "               -4.46327209e-01,  5.38869314e-02],\n",
              "              [ 3.39403033e-01, -2.29931191e-01, -4.99022007e-02,\n",
              "               -4.42490160e-01,  5.53749502e-02],\n",
              "              [ 3.19989771e-01, -2.12421790e-01, -3.71040255e-02,\n",
              "               -4.00347352e-01,  4.40392122e-02],\n",
              "              [ 3.37042958e-01, -2.26151302e-01, -5.11298105e-02,\n",
              "               -4.44833457e-01,  5.99641390e-02],\n",
              "              [ 3.41690034e-01, -2.31731117e-01, -4.86783087e-02,\n",
              "               -4.47686166e-01,  5.31994589e-02]],\n",
              "\n",
              "             [[ 2.92921066e-01, -1.55885398e-01, -7.77644515e-02,\n",
              "               -5.90915382e-01, -3.11172158e-02],\n",
              "              [ 2.99603164e-01, -1.54758394e-01, -7.22781718e-02,\n",
              "               -5.84227681e-01, -3.78553197e-02],\n",
              "              [ 2.94318765e-01, -1.57750577e-01, -7.42170960e-02,\n",
              "               -5.85073829e-01, -3.66292968e-02],\n",
              "              [ 2.97444344e-01, -1.55028641e-01, -7.62576610e-02,\n",
              "               -5.86965799e-01, -3.37135792e-02],\n",
              "              [ 2.50750005e-01, -1.44833237e-01, -6.86514676e-02,\n",
              "               -5.20518780e-01, -3.72312367e-02],\n",
              "              [ 2.96358198e-01, -1.54146165e-01, -7.57152289e-02,\n",
              "               -5.87224901e-01, -3.20609882e-02],\n",
              "              [ 2.99008280e-01, -1.54093504e-01, -7.42060095e-02,\n",
              "               -5.88312805e-01, -3.36135104e-02],\n",
              "              [ 2.63702452e-01, -1.38808459e-01, -5.98222613e-02,\n",
              "               -5.47412634e-01, -5.31209260e-03],\n",
              "              [ 2.98592716e-01, -1.56710461e-01, -7.59601146e-02,\n",
              "               -5.84881186e-01, -3.63671444e-02],\n",
              "              [ 2.99737036e-01, -1.56356215e-01, -7.35307038e-02,\n",
              "               -5.84844589e-01, -3.85847464e-02]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYeS4TJhTvQZ"
      },
      "source": [
        "**LSH Self-Atention**\n",
        "\n",
        "* uses q's only (no k's)\n",
        "* calculates similarity of each q relative to all other q's\n",
        "* uses bucketing\n",
        "* generates multiple hash tables\n",
        "* dot product is generated only between members of the bucket\n",
        "* we get a reduced dot-product attention array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4G32jVWPEBK",
        "outputId": "1d64ca3d-c0e8-40fb-acfd-54bbf37629c9"
      },
      "source": [
        "def hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False):\n",
        "    # check for even, integer bucket sizes\n",
        "    assert isinstance(n_buckets, int) and n_buckets % 2 == 0\n",
        "\n",
        "    rng = fastmath.stop_gradient(tie_in(vecs, rng))\n",
        "    rot_size = n_buckets\n",
        "\n",
        "    rotations_shape = [vecs.shape[-1], n_hashes, rot_size//2]\n",
        "    random_rotations = fastmath.random.normal(rng, rotations_shape).astype(np.float32)\n",
        "    if verbose:\n",
        "        print(\"random.rotations.shape\", random_rotations.shape)\n",
        "\n",
        "    if fastmath.backend_name() == \"jax\":\n",
        "        rotated_vecs = np.einsum(\"tf,fhb->htb\", vecs, random_rotations)\n",
        "    else:\n",
        "        random_rotations = np.reshape(random_rotations, [-1, n_hashes * (rot_size // 2)])\n",
        "        if verbose:\n",
        "            print(\"random_rotations reshaped\", random_rotations.shape)\n",
        "        rotated_vecs = np.dot(vecs, random_rotations)\n",
        "        if verbose:\n",
        "            print(\"rotated_vecs1\", rotated_vecs.shape)\n",
        "        rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])\n",
        "        if verbose:\n",
        "            print(\"rotated_vecs2\", rotated_vecs.shape)\n",
        "        rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))\n",
        "        if verbose:\n",
        "            print(\"rotated_vecs3\", rotated_vecs.shape)\n",
        "\n",
        "    rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)\n",
        "    if verbose:\n",
        "        print(\"rotated_vecs.shape\", rotated_vecs.shape)\n",
        "    buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)\n",
        "    if verbose:\n",
        "        print(\"buckets.shape\", buckets.shape)\n",
        "    if verbose:\n",
        "        print(\"buckets\", buckets)\n",
        "\n",
        "    if mask is not None:\n",
        "        n_buckets += 1  # Create an extra bucket for padding tokens only\n",
        "        buckets = np.where(mask[None, :], buckets, n_buckets - 1)\n",
        "\n",
        "    # buckets is now (n_hashes, seqlen)\n",
        "    # offsets is needed for bucket numbers from different hashing rounds not to overlap\n",
        "    offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))\n",
        "    offsets = np.reshape(offsets * n_buckets, (-1, 1))\n",
        "    size = n_hashes * vecs.shape[0]\n",
        "    buckets = np.reshape(buckets + offsets, (-1,))\n",
        "    if verbose:\n",
        "        print(\"buckets with offsets\", buckets.shape, \"\\n\", buckets)\n",
        "    return buckets\n",
        "\n",
        "\n",
        "ohv_q = np.ones((10, 5))  \n",
        "ohv_n_buckets = 6  # even number\n",
        "ohv_n_hashes = 3\n",
        "with fastmath.use_backend(\"tensorflow-numpy\"):\n",
        "    ohv_rng = fastmath.random.get_prng(1)\n",
        "    ohv = hash_vectors(\n",
        "        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n",
        "    )\n",
        "    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n",
        "# the random number generators do not produce the same results with different backends\n",
        "with fastmath.use_backend(\"jax\"):\n",
        "    ohv_rng = fastmath.random.get_prng(1)\n",
        "    ohv = hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None)\n",
        "    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random.rotations.shape (5, 3, 3)\n",
            "random_rotations reshaped (5, 9)\n",
            "rotated_vecs1 (10, 9)\n",
            "rotated_vecs2 (10, 3, 3)\n",
            "rotated_vecs3 (3, 10, 3)\n",
            "rotated_vecs.shape (3, 10, 6)\n",
            "buckets.shape (3, 10)\n",
            "buckets ndarray<tf.Tensor(\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [3 3 3 3 3 3 3 3 3 3]\n",
            " [5 5 5 5 5 5 5 5 5 5]], shape=(3, 10), dtype=int32)>\n",
            "buckets with offsets (30,) \n",
            " ndarray<tf.Tensor(\n",
            "[ 0  0  0  0  0  0  0  0  0  0  9  9  9  9  9  9  9  9  9  9 17 17 17 17\n",
            " 17 17 17 17 17 17], shape=(30,), dtype=int32)>\n",
            "ohv shape (30,) \n",
            "ohv ndarray<tf.Tensor(\n",
            "[ 0  0  0  0  0  0  0  0  0  0  9  9  9  9  9  9  9  9  9  9 17 17 17 17\n",
            " 17 17 17 17 17 17], shape=(30,), dtype=int32)>\n",
            "ohv shape (30,) \n",
            "ohv [ 0  0  0  0  0  0  0  0  0  0 11 11 11 11 11 11 11 11 11 11 15 15 15 15\n",
            " 15 15 15 15 15 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS-61hj0hLJt"
      },
      "source": [
        "`q[n_seq,n_q]`\n",
        "`n_hash = 2`\n",
        "`n_buckets = 4`\n",
        "`n_seq = 8`\n",
        "\n",
        "`bucket = [0,1,2,3,0,1,2,3, 4,5,6,7,4,5,6,7]`\n",
        "\n",
        "Bucket is `n_hash*n_seq` long, the bucket values are offset by `n_hash` (the numbers do not overlap)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5BsQUGngyul",
        "outputId": "da97546e-66c3-4beb-e51c-f13ec4199618"
      },
      "source": [
        "def sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"---sort_buckets--\")\n",
        "    ticker = np.arange(n_hashes*seqlen)\n",
        "    if verbose:\n",
        "        print(\"ticker\", ticker.shape, ticker)\n",
        "    buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
        "    if verbose:\n",
        "        print(\"buckets_and_t\", buckets_and_t.shape, buckets_and_t)\n",
        "\n",
        "    # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
        "    sorted_buckets_and_t, sorted_ticker = fastmath.sort_key_val(buckets_and_t, ticker, dimension=-1)\n",
        "    if verbose:\n",
        "        print(\"sorted_buckets_and_t\", sorted_buckets_and_t.shape, sorted_buckets_and_t)\n",
        "    if verbose:\n",
        "        print(\"sorted_ticker\", sorted_ticker.shape, sorted_ticker)\n",
        "    _, undo_sort = fastmath.sort_key_val(sorted_ticker, ticker, dimension=-1)\n",
        "    if verbose:\n",
        "        print(\"undo_sort\", undo_sort.shape, undo_sort)\n",
        "\n",
        "    st = sorted_ticker % seqlen\n",
        "    sq = np.take(q, st, axis=0)\n",
        "    sv = np.take(v, st, axis=0)\n",
        "    return sq, sv, sorted_ticker, undo_sort\n",
        "\n",
        "\n",
        "t_n_hashes = 2\n",
        "t_n_buckets = 6\n",
        "t_n_seq = t_seqlen = 10\n",
        "t_n_q = 3\n",
        "n_v = 5\n",
        "\n",
        "t_q = (np.array([(j % t_n_buckets) for j in range(t_n_seq)]) * np.ones((t_n_q, 1))).T\n",
        "t_v = np.ones((t_n_seq, n_v))\n",
        "t_buckets = np.array(\n",
        "    [\n",
        "        (j % t_n_buckets) + t_n_buckets * i\n",
        "        for i in range(t_n_hashes)\n",
        "        for j in range(t_n_seq)\n",
        "    ]\n",
        ")\n",
        "print(\"q\\n\", t_q)\n",
        "print(\"t_buckets: \", t_buckets)\n",
        "\n",
        "t_sq, t_sv, t_sticker, t_undo_sort = sort_buckets(\n",
        "    t_buckets, t_q, t_v, t_n_buckets, t_n_hashes, t_seqlen, verbose=True\n",
        ")\n",
        "\n",
        "print(\"sq.shape\", t_sq.shape, \"sv.shape\", t_sv.shape)\n",
        "print(\"sq\\n\", t_sq)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q\n",
            " [[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [2. 2. 2.]\n",
            " [3. 3. 3.]\n",
            " [4. 4. 4.]\n",
            " [5. 5. 5.]\n",
            " [0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [2. 2. 2.]\n",
            " [3. 3. 3.]]\n",
            "t_buckets:  [ 0  1  2  3  4  5  0  1  2  3  6  7  8  9 10 11  6  7  8  9]\n",
            "---sort_buckets--\n",
            "ticker (20,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "buckets_and_t (20,) [  0  11  22  33  44  55   6  17  28  39  60  71  82  93 104 115  66  77\n",
            "  88  99]\n",
            "sorted_buckets_and_t (20,) [  0   6  11  17  22  28  33  39  44  55  60  66  71  77  82  88  93  99\n",
            " 104 115]\n",
            "sorted_ticker (20,) [ 0  6  1  7  2  8  3  9  4  5 10 16 11 17 12 18 13 19 14 15]\n",
            "undo_sort (20,) [ 0  2  4  6  8  9  1  3  5  7 10 12 14 16 18 19 11 13 15 17]\n",
            "sq.shape (20, 3) sv.shape (20, 5)\n",
            "sq\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [4. 4. 4.]\n",
            " [5. 5. 5.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [3. 3. 3.]\n",
            " [3. 3. 3.]\n",
            " [4. 4. 4.]\n",
            " [5. 5. 5.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07mP1uuBoSG-"
      },
      "source": [
        "def dotandv(\n",
        "    sorted_q, sorted_v, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose=False\n",
        "):\n",
        "            \n",
        "    reshaped_sorted_q = np.reshape(sorted_q, (-1, kv_chunk_len, sorted_q.shape[-1]))\n",
        "    reshaped_sorted_qt = np.swapaxes(reshaped_sorted_q, -1, -2)\n",
        "    if verbose:\n",
        "        print(\"rsorted_q.shape,reshaped_sorted_qt.shape: \", reshaped_sorted_q.shape, reshaped_sorted_qt.shape)\n",
        "    dotlike = np.matmul(reshaped_sorted_q, reshaped_sorted_qt)\n",
        "    if verbose:\n",
        "        print(\"dotlike\\n\", dotlike)\n",
        "\n",
        "    dotlike, sorted_logits = softmax(dotlike, passthrough)\n",
        "    if verbose:\n",
        "        print(\"dotlike post softmax\\n\", dotlike)\n",
        "\n",
        "    vr = np.reshape(sorted_v, (-1, kv_chunk_len, sorted_v.shape[-1]))\n",
        "    if verbose:\n",
        "        print(\"dotlike.shape, vr.shape:\", dotlike.shape, vr.shape)\n",
        "    so = np.matmul(dotlike, vr)\n",
        "    if verbose:\n",
        "        print(\"so.shape:\", so.shape)\n",
        "    so = np.reshape(so, (-1, so.shape[-1]))\n",
        "    sorted_logits = np.reshape(sorted_logits, (-1,))  # provided\n",
        "    if verbose:\n",
        "        print(\"so.shape,sorted_logits.shape\", so.shape, sorted_logits.shape)\n",
        "\n",
        "    o = np.take(so, undo_sort, axis=0)\n",
        "    logits = np.take(sorted_logits, undo_sort, axis=0)\n",
        "    if verbose:\n",
        "        print(\"o.shape,o\", o.shape, o)\n",
        "    if verbose:\n",
        "        print(\"logits.shape, logits\", logits.shape, logits)\n",
        "\n",
        "    if n_hashes > 1:\n",
        "        o = np.reshape(o, (n_hashes, seqlen, o.shape[-1]))\n",
        "        logits = np.reshape(logits, (n_hashes, seqlen, 1))\n",
        "        probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n",
        "        o = np.sum(o * probs, axis=0)\n",
        "\n",
        "    return o"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt88QVHdVoB5",
        "outputId": "d5280323-acf9-4899-8210-d0383388f3ed"
      },
      "source": [
        "t_kv_chunk_len = 2\n",
        "out = dotandv(\n",
        "    t_sq,\n",
        "    t_sv,\n",
        "    t_undo_sort,\n",
        "    t_kv_chunk_len,\n",
        "    t_n_hashes,\n",
        "    t_seqlen,\n",
        "    passthrough=True,\n",
        "    verbose=True,\n",
        ")\n",
        "print(\"out\\n\", out)\n",
        "print(\"\\n-----With softmax enabled----\\n\")\n",
        "out = dotandv(\n",
        "    t_sq,\n",
        "    t_sv,\n",
        "    t_undo_sort,\n",
        "    t_kv_chunk_len,\n",
        "    t_n_hashes,\n",
        "    t_seqlen,\n",
        "    passthrough=False,\n",
        "    verbose=True,\n",
        ")\n",
        "print(\"out\\n\", out)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rsorted_q.shape,reshaped_sorted_qt.shape:  (10, 2, 3) (10, 3, 2)\n",
            "dotlike\n",
            " [[[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]\n",
            "\n",
            " [[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]]\n",
            "dotlike post softmax\n",
            " [[[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]\n",
            "\n",
            " [[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]]\n",
            "dotlike.shape, vr.shape: (10, 2, 2) (10, 2, 5)\n",
            "so.shape: (10, 2, 5)\n",
            "so.shape,sorted_logits.shape (20, 5) (20,)\n",
            "o.shape,o (20, 5) [[  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]\n",
            " [108. 108. 108. 108. 108.]\n",
            " [135. 135. 135. 135. 135.]\n",
            " [  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]\n",
            " [  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]\n",
            " [108. 108. 108. 108. 108.]\n",
            " [135. 135. 135. 135. 135.]\n",
            " [  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]]\n",
            "logits.shape, logits (20,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "out\n",
            " [[  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]\n",
            " [108. 108. 108. 108. 108.]\n",
            " [135. 135. 135. 135. 135.]\n",
            " [  0.   0.   0.   0.   0.]\n",
            " [  6.   6.   6.   6.   6.]\n",
            " [ 24.  24.  24.  24.  24.]\n",
            " [ 54.  54.  54.  54.  54.]]\n",
            "\n",
            "-----With softmax enabled----\n",
            "\n",
            "rsorted_q.shape,reshaped_sorted_qt.shape:  (10, 2, 3) (10, 3, 2)\n",
            "dotlike\n",
            " [[[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]\n",
            "\n",
            " [[ 0.  0.]\n",
            "  [ 0.  0.]]\n",
            "\n",
            " [[ 3.  3.]\n",
            "  [ 3.  3.]]\n",
            "\n",
            " [[12. 12.]\n",
            "  [12. 12.]]\n",
            "\n",
            " [[27. 27.]\n",
            "  [27. 27.]]\n",
            "\n",
            " [[48. 60.]\n",
            "  [60. 75.]]]\n",
            "dotlike post softmax\n",
            " [[[5.0000000e-01 5.0000000e-01]\n",
            "  [5.0000000e-01 5.0000000e-01]]\n",
            "\n",
            " [[5.0000000e-01 5.0000000e-01]\n",
            "  [5.0000000e-01 5.0000000e-01]]\n",
            "\n",
            " [[4.9999976e-01 4.9999976e-01]\n",
            "  [4.9999976e-01 4.9999976e-01]]\n",
            "\n",
            " [[4.9999976e-01 4.9999976e-01]\n",
            "  [4.9999976e-01 4.9999976e-01]]\n",
            "\n",
            " [[6.1441656e-06 9.9999237e-01]\n",
            "  [3.0590232e-07 1.0000000e+00]]\n",
            "\n",
            " [[5.0000000e-01 5.0000000e-01]\n",
            "  [5.0000000e-01 5.0000000e-01]]\n",
            "\n",
            " [[5.0000000e-01 5.0000000e-01]\n",
            "  [5.0000000e-01 5.0000000e-01]]\n",
            "\n",
            " [[4.9999976e-01 4.9999976e-01]\n",
            "  [4.9999976e-01 4.9999976e-01]]\n",
            "\n",
            " [[4.9999976e-01 4.9999976e-01]\n",
            "  [4.9999976e-01 4.9999976e-01]]\n",
            "\n",
            " [[6.1441656e-06 9.9999237e-01]\n",
            "  [3.0590232e-07 1.0000000e+00]]]\n",
            "dotlike.shape, vr.shape: (10, 2, 2) (10, 2, 5)\n",
            "so.shape: (10, 2, 5)\n",
            "so.shape,sorted_logits.shape (20, 5) (20,)\n",
            "o.shape,o (20, 5) [[1.        1.        1.        1.        1.       ]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999985 0.9999985 0.9999985 0.9999985 0.9999985]\n",
            " [1.0000004 1.0000004 1.0000004 1.0000004 1.0000004]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999985 0.9999985 0.9999985 0.9999985 0.9999985]\n",
            " [1.0000004 1.0000004 1.0000004 1.0000004 1.0000004]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [1.        1.        1.        1.        1.       ]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
            " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]\n",
            "logits.shape, logits (20,) [ 0.6931472  3.6931472 12.693148  27.693148  60.000008  75.\n",
            "  0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472\n",
            " 12.693148  27.693148  60.000008  75.         0.6931472  3.6931472\n",
            " 12.693148  27.693148 ]\n",
            "out\n",
            " [[1.         1.         1.         1.         1.        ]\n",
            " [1.         1.         1.         1.         1.        ]\n",
            " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
            " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
            " [0.99999994 0.99999994 0.99999994 0.99999994 0.99999994]\n",
            " [1.0000018  1.0000018  1.0000018  1.0000018  1.0000018 ]\n",
            " [1.         1.         1.         1.         1.        ]\n",
            " [1.         1.         1.         1.         1.        ]\n",
            " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
            " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLvO2lUNVsjV"
      },
      "source": [
        "# original version from trax 1.3.4\n",
        "def attend(\n",
        "    q,\n",
        "    k=None,\n",
        "    v=None,\n",
        "    q_chunk_len=None,\n",
        "    kv_chunk_len=None,\n",
        "    n_chunks_before=0,\n",
        "    n_chunks_after=0,\n",
        "    mask_fn=None,\n",
        "    q_info=None,\n",
        "    kv_info=None,\n",
        "    dropout=0.0,\n",
        "    rng=None,\n",
        "):\n",
        "    \"\"\"Dot-product attention, with optional chunking and/or masking.\n",
        "\n",
        "  Args:\n",
        "    q: Query vectors, shape [q_len, d_qk]\n",
        "    k: Key vectors, shape [kv_len, d_qk]; or None\n",
        "    v: Value vectors, shape [kv_len, d_v]\n",
        "    q_chunk_len: Set to non-zero to enable chunking for query vectors\n",
        "    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors\n",
        "    n_chunks_before: Number of adjacent previous chunks to attend to\n",
        "    n_chunks_after: Number of adjacent subsequent chunks to attend to\n",
        "    mask_fn: TODO(kitaev) doc\n",
        "    q_info: Query-associated metadata for masking\n",
        "    kv_info: Key-associated metadata for masking\n",
        "    dropout: Dropout rate\n",
        "    rng: RNG for dropout\n",
        "\n",
        "  Returns:\n",
        "    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n",
        "    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n",
        "    probabilities is useful for combining multiple rounds of attention (as in\n",
        "    LSH attention).\n",
        "  \"\"\"\n",
        "    assert v is not None\n",
        "    share_qk = k is None\n",
        "\n",
        "    if q_info is None:\n",
        "        q_info = np.arange(q.shape[-2], dtype=np.int32)\n",
        "\n",
        "    if kv_info is None and not share_qk:\n",
        "        kv_info = np.arange(v.shape[-2], dtype=np.int32)\n",
        "\n",
        "    # Split q/k/v into chunks along the time axis, if desired.\n",
        "    if q_chunk_len is not None:\n",
        "        q = np.reshape(q, (-1, q_chunk_len, q.shape[-1]))\n",
        "        q_info = np.reshape(q_info, (-1, q_chunk_len))\n",
        "\n",
        "    if share_qk:\n",
        "        assert kv_chunk_len is None or kv_chunk_len == q_chunk_len\n",
        "        k = q\n",
        "        kv_chunk_len = q_chunk_len\n",
        "        if kv_info is None:\n",
        "            kv_info = q_info\n",
        "        elif kv_chunk_len is not None:\n",
        "            # kv_info is not None, but reshape as required.\n",
        "            kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n",
        "    elif kv_chunk_len is not None:\n",
        "        k = np.reshape(k, (-1, kv_chunk_len, k.shape[-1]))\n",
        "        kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n",
        "\n",
        "    if kv_chunk_len is not None:\n",
        "        v = np.reshape(v, (-1, kv_chunk_len, v.shape[-1]))\n",
        "\n",
        "    if share_qk:\n",
        "        k = length_normalized(k)\n",
        "    k = k / np.sqrt(k.shape[-1])\n",
        "\n",
        "    # Optionally include adjacent chunks.\n",
        "    if q_chunk_len is not None or kv_chunk_len is not None:\n",
        "        assert q_chunk_len is not None and kv_chunk_len is not None\n",
        "    else:\n",
        "        assert n_chunks_before == 0 and n_chunks_after == 0\n",
        "\n",
        "    k = look_adjacent(k, n_chunks_before, n_chunks_after)\n",
        "    v = look_adjacent(v, n_chunks_before, n_chunks_after)\n",
        "    kv_info = look_adjacent(kv_info, n_chunks_before, n_chunks_after)\n",
        "\n",
        "    # Dot-product attention.\n",
        "    dots = np.matmul(q, np.swapaxes(k, -1, -2))\n",
        "\n",
        "    # Masking\n",
        "    if mask_fn is not None:\n",
        "        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n",
        "\n",
        "    # Softmax.\n",
        "    dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "    dots = np.exp(dots - dots_logsumexp)\n",
        "\n",
        "    if dropout > 0.0:\n",
        "        assert rng is not None\n",
        "        # Dropout is broadcast across the bin dimension\n",
        "        dropout_shape = (dots.shape[-2], dots.shape[-1])\n",
        "        #\n",
        "        keep_prob = tie_in(dots, 1.0 - dropout)\n",
        "        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n",
        "        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n",
        "        dots = dots * multiplier\n",
        "\n",
        "    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n",
        "    out = np.matmul(dots, v)\n",
        "    out = np.reshape(out, (-1, out.shape[-1]))\n",
        "    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n",
        "    return out, dots_logsumexp"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4AJ6bjYY4fS",
        "outputId": "f47d871e-7a03-41da-cb20-75f359462ce2"
      },
      "source": [
        "class LSHSelfAttention(tl.LSHSelfAttention):\n",
        "\n",
        "    def forward_unbatched(self, x, mask=None, *, weights, state, rng, update_state):\n",
        "        attend_rng, output_rng = fastmath.random.split(rng)\n",
        "        w_q, w_v, w_o = weights\n",
        "\n",
        "        q = np.matmul(x, w_q)\n",
        "        v = np.matmul(x, w_v)\n",
        "\n",
        "        if update_state:\n",
        "            _, old_hash_rng = state\n",
        "            hash_rng, hash_subrng = fastmath.random.split(old_hash_rng)\n",
        "            buckets = hash_vectors(\n",
        "                q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n",
        "            )\n",
        "            s_buckets = buckets\n",
        "            if self._max_length_for_buckets:\n",
        "                length = self.n_hashes * self._max_length_for_buckets\n",
        "                if buckets.shape[0] < length:\n",
        "                    s_buckets = np.concatenate(\n",
        "                        [buckets, np.zeros(length - buckets.shape[0], dtype=np.int32)],\n",
        "                        axis=0,\n",
        "                    )\n",
        "            state = (s_buckets, hash_rng)\n",
        "        else:\n",
        "            buckets, _ = state\n",
        "            if self._max_length_for_buckets:\n",
        "                buckets = buckets[: self.n_hashes * x.shape[0]]\n",
        "\n",
        "        seqlen = x.shape[0]\n",
        "        assert int(buckets.shape[0]) == self.n_hashes * seqlen\n",
        "\n",
        "        ticker = tie_in(x, np.arange(self.n_hashes * seqlen, dtype=np.int32))\n",
        "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
        "        buckets_and_t = fastmath.stop_gradient(buckets_and_t)\n",
        "\n",
        "        sbuckets_and_t, sticker = fastmath.sort_key_val(\n",
        "            buckets_and_t, ticker, dimension=-1\n",
        "        )\n",
        "        _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n",
        "        sbuckets_and_t = fastmath.stop_gradient(sbuckets_and_t)\n",
        "        sticker = fastmath.stop_gradient(sticker)\n",
        "        undo_sort = fastmath.stop_gradient(undo_sort)\n",
        "\n",
        "        st = sticker % seqlen\n",
        "        sq = np.take(q, st, axis=0)\n",
        "        sv = np.take(v, st, axis=0)\n",
        "\n",
        "        mask_fn = functools.partial(\n",
        "            mask_self_attention,\n",
        "            causal=self.causal,\n",
        "            exclude_self=True,\n",
        "            masked=self.masked,\n",
        "        )\n",
        "        q_info = st\n",
        "\n",
        "        assert (mask is not None) == self.masked\n",
        "        kv_info = None\n",
        "        if self.masked:\n",
        "            smask = np.take(mask, st, axis=0)\n",
        "            ones_like_mask = tie_in(x, np.ones_like(smask, dtype=np.int32))\n",
        "            kv_info = q_info * np.where(smask, ones_like_mask, -ones_like_mask)\n",
        "\n",
        "        so, slogits = attend(\n",
        "            sq,\n",
        "            k=None,\n",
        "            v=sv,\n",
        "            q_chunk_len=self.chunk_len,\n",
        "            n_chunks_before=self.n_chunks_before,\n",
        "            n_chunks_after=self.n_chunks_after,\n",
        "            mask_fn=mask_fn,\n",
        "            q_info=q_info,\n",
        "            kv_info=kv_info,\n",
        "            dropout=self.attention_dropout,\n",
        "            rng=attend_rng,\n",
        "        )\n",
        "\n",
        "        o = permute_via_gather(so, undo_sort, sticker, axis=0)\n",
        "        logits = permute_via_sort(slogits, sticker, buckets_and_t, axis=-1)\n",
        "\n",
        "        if self.n_hashes > 1:\n",
        "            o = np.reshape(o, (self.n_hashes, seqlen, o.shape[-1]))\n",
        "            logits = np.reshape(logits, (self.n_hashes, seqlen, 1))\n",
        "            probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n",
        "            o = np.sum(o * probs, axis=0)\n",
        "\n",
        "        assert o.shape == (seqlen, w_v.shape[-1])\n",
        "        out = np.matmul(o, w_o)\n",
        "        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n",
        "        return out, state\n",
        "\n",
        "\n",
        "# Here we're going to try out our LSHSelfAttention\n",
        "n_heads = 3\n",
        "causal = False\n",
        "masked = False\n",
        "mask = None\n",
        "chunk_len = 10\n",
        "n_chunks_before = 0\n",
        "n_chunks_after = 0\n",
        "attention_dropout = 0.0\n",
        "n_hashes = 5\n",
        "n_buckets = 4\n",
        "seq_len = 10\n",
        "emb_len = 3\n",
        "al = LSHSelfAttention(\n",
        "    n_heads=n_heads,\n",
        "    d_qk=3,\n",
        "    d_v=4,\n",
        "    causal=causal,\n",
        "    chunk_len=10,\n",
        "    n_chunks_before=n_chunks_before,\n",
        "    n_chunks_after=n_chunks_after,\n",
        "    n_hashes=n_hashes,\n",
        "    n_buckets=n_buckets,\n",
        "    use_reference_code=True,\n",
        "    attention_dropout=attention_dropout,\n",
        "    mode=\"train\",\n",
        ")\n",
        "\n",
        "x = jax.random.uniform(jax.random.PRNGKey(0), (1, seq_len, emb_len), dtype=np.float32)\n",
        "al_osa = fastmath.random.get_prng(1)\n",
        "_, _ = al.init(tl.shapes.signature(x), rng=al_osa)\n",
        "\n",
        "al(x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[-0.1887156 , -0.4345857 , -0.30507997],\n",
              "              [-0.17001462, -0.3992369 , -0.27571315],\n",
              "              [-0.18278453, -0.42552638, -0.31050825],\n",
              "              [-0.19504203, -0.42496088, -0.30478832],\n",
              "              [-0.17509116, -0.40555453, -0.28232577],\n",
              "              [-0.17590663, -0.3963362 , -0.29975712],\n",
              "              [-0.2093995 , -0.45383382, -0.2939606 ],\n",
              "              [-0.19098149, -0.43987733, -0.29390997],\n",
              "              [-0.2013992 , -0.44397017, -0.32320336],\n",
              "              [-0.20334871, -0.45338914, -0.3046518 ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}